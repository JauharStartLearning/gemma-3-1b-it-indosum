{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11211383,"sourceType":"datasetVersion","datasetId":7000625},{"sourceId":11265627,"sourceType":"datasetVersion","datasetId":7041784}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install seacrowd accelerate peft bitsandbytes wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:44:10.235743Z","iopub.execute_input":"2025-04-06T07:44:10.236012Z","iopub.status.idle":"2025-04-06T07:44:33.145303Z","shell.execute_reply.started":"2025-04-06T07:44:10.235976Z","shell.execute_reply":"2025-04-06T07:44:33.144207Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nCollecting seacrowd\n  Downloading seacrowd-0.2.2-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nCollecting loguru>=0.5.3 (from seacrowd)\n  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\nCollecting bioc>=1.3.7 (from seacrowd)\n  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: pandas>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (2.2.3)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (1.26.4)\nRequirement already satisfied: datasets>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.3.1)\nCollecting black~=22.0 (from seacrowd)\n  Downloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (52 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting flake8>=3.8.3 (from seacrowd)\n  Downloading flake8-7.2.0-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting isort>=5.0.0 (from seacrowd)\n  Downloading isort-6.0.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.11.12)\nCollecting pre-commit>=2.19.0 (from seacrowd)\n  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting jsonlines>=3.1.0 (from seacrowd)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: torchaudio>=0.11 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (2.5.1+cu121)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from seacrowd) (0.12.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from seacrowd) (0.10.2.post1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.2.4)\nCollecting zstandard (from seacrowd)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting ffmpeg (from seacrowd)\n  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting conllu (from seacrowd)\n  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.1.5)\nCollecting translate-toolkit>=3.7.3 (from seacrowd)\n  Downloading translate_toolkit-3.15.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from seacrowd) (4.12.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (1.18.3)\nRequirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.10/dist-packages (from bioc>=1.3.7->seacrowd) (5.3.0)\nCollecting intervaltree (from bioc>=1.3.7->seacrowd)\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting docopt (from bioc>=1.3.7->seacrowd)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black~=22.0->seacrowd) (1.0.0)\nCollecting pathspec>=0.9.0 (from black~=22.0->seacrowd)\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black~=22.0->seacrowd) (2.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.2.0->seacrowd) (2024.12.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nCollecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.8.3->seacrowd)\n  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\nCollecting pycodestyle<2.14.0,>=2.13.0 (from flake8>=3.8.3->seacrowd)\n  Downloading pycodestyle-2.13.0-py2.py3-none-any.whl.metadata (4.5 kB)\nCollecting pyflakes<3.4.0,>=3.3.0 (from flake8>=3.8.3->seacrowd)\n  Downloading pyflakes-3.3.2-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.3->seacrowd) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.3->seacrowd) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.3->seacrowd) (2025.1)\nCollecting cfgv>=2.0.0 (from pre-commit>=2.19.0->seacrowd)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify>=1.0.0 (from pre-commit>=2.19.0->seacrowd)\n  Downloading identify-2.6.9-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv>=0.11.1 (from pre-commit>=2.19.0->seacrowd)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nCollecting virtualenv>=20.10.0 (from pre-commit>=2.19.0->seacrowd)\n  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nCollecting cwcwidth<0.2,>=0.1.10 (from translate-toolkit>=3.7.3->seacrowd)\n  Downloading cwcwidth-0.1.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (0.60.0)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.1.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->seacrowd) (1.17.1)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->seacrowd) (2.0.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->seacrowd) (2.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->seacrowd) (0.43.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->seacrowd) (3.5.0)\nCollecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=2.19.0->seacrowd)\n  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree->bioc>=1.3.7->seacrowd) (2.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20->seacrowd) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20->seacrowd) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->seacrowd) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20->seacrowd) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20->seacrowd) (2024.2.0)\nDownloading seacrowd-0.2.2-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\nDownloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading flake8-7.2.0-py2.py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading isort-6.0.1-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading translate_toolkit-3.15.1-py3-none-any.whl (744 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m744.9/744.9 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading conllu-6.0.0-py3-none-any.whl (16 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading cwcwidth-0.1.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading identify-2.6.9-py2.py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\nDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading pycodestyle-2.13.0-py2.py3-none-any.whl (31 kB)\nDownloading pyflakes-3.3.2-py2.py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ffmpeg, docopt, intervaltree\n  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=0c2b984873dd03a78f07994079c2d0c5db2aa85a8766a97f50acbbc7edcfb35d\n  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=937eca6e0e58b09287b29bac971c77ada8987aca2fa92d12916d279f005fb923\n  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=c9a38bff843bfb5794cc259520e847628e1c9430b8045757056a0888366b3adf\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\nSuccessfully built ffmpeg docopt intervaltree\nInstalling collected packages: ffmpeg, docopt, distlib, zstandard, virtualenv, pyflakes, pycodestyle, pathspec, nodeenv, mccabe, loguru, jsonlines, isort, intervaltree, identify, cwcwidth, conllu, cfgv, translate-toolkit, pre-commit, flake8, black, bioc, seacrowd, bitsandbytes\nSuccessfully installed bioc-2.1 bitsandbytes-0.45.4 black-22.12.0 cfgv-3.4.0 conllu-6.0.0 cwcwidth-0.1.10 distlib-0.3.9 docopt-0.6.2 ffmpeg-1.4 flake8-7.2.0 identify-2.6.9 intervaltree-3.1.0 isort-6.0.1 jsonlines-4.0.0 loguru-0.7.3 mccabe-0.7.0 nodeenv-1.9.1 pathspec-0.12.1 pre-commit-4.2.0 pycodestyle-2.13.0 pyflakes-3.3.2 seacrowd-0.2.2 translate-toolkit-3.15.1 virtualenv-20.30.0 zstandard-0.23.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install unsloth vllm\n!pip install triton==3.1.0\n!pip install -U pynvml\n# Install latest Hugging Face for Gemma-3!\n!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:44:33.146441Z","iopub.execute_input":"2025-04-06T07:44:33.146693Z","iopub.status.idle":"2025-04-06T07:49:45.992153Z","shell.execute_reply.started":"2025-04-06T07:44:33.146671Z","shell.execute_reply":"2025-04-06T07:49:45.990742Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random \nimport torch\nimport datasets\n\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:45.993539Z","iopub.execute_input":"2025-04-06T07:49:45.993927Z","iopub.status.idle":"2025-04-06T07:49:51.697954Z","shell.execute_reply.started":"2025-04-06T07:49:45.993889Z","shell.execute_reply":"2025-04-06T07:49:51.697135Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Detect train, dev, and test files\nDATASET_ROOT = '/kaggle/input/indosum/indosum'\n\nfiles_id_dir = os.listdir(DATASET_ROOT)\ntrain_files = []\ndev_files = []\ntest_files = []\n\nfor filename in files_id_dir:\n    if 'train' in filename:\n        train_files.append(filename)\n    elif 'dev' in filename:\n        dev_files.append(filename)\n    elif 'test' in filename:\n        test_files.append(filename)\n\ntrain_files, dev_files, test_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:51.700004Z","iopub.execute_input":"2025-04-06T07:49:51.700510Z","iopub.status.idle":"2025-04-06T07:49:51.745637Z","shell.execute_reply.started":"2025-04-06T07:49:51.700482Z","shell.execute_reply":"2025-04-06T07:49:51.744906Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(['train.01.jsonl',\n  'train.05.jsonl',\n  'train.03.jsonl',\n  'train.04.jsonl',\n  'train.02.jsonl'],\n ['dev.01.jsonl',\n  'dev.05.jsonl',\n  'dev.04.jsonl',\n  'dev.03.jsonl',\n  'dev.02.jsonl'],\n ['test.05.jsonl',\n  'test.04.jsonl',\n  'test.02.jsonl',\n  'test.03.jsonl',\n  'test.01.jsonl'])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_files = ['train.01.jsonl']\ntest_files = ['test.01.jsonl']\ndev_files = ['dev.01.jsonl']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:51.746884Z","iopub.execute_input":"2025-04-06T07:49:51.747190Z","iopub.status.idle":"2025-04-06T07:49:51.750622Z","shell.execute_reply.started":"2025-04-06T07:49:51.747169Z","shell.execute_reply":"2025-04-06T07:49:51.749862Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_file_to_json_list(filename):\n    file = os.path.join(DATASET_ROOT, filename)\n    data = []\n    with open(file, 'r') as f:\n        json_list = list(f)\n        for json_str in tqdm(json_list, desc=f'Loading data {filename}'):\n            d = json.loads(json_str)\n            data.append(d)\n    return data\n\ndef label_to_dict_str(label_list):\n    label_dict = {} # key = paragraph_id : value = label list \n    for i, label in enumerate(label_list[:]):\n        label_dict[i] = label\n\n    json_str = json.dumps(label_dict)\n    num = len(label_dict)\n    return json_str, num\n\ndef paragraph_to_dict_str(paragraph_list):\n    paragraph_dict = {} # key = paragraph_id : value = paragraph list \n    for i, paragraph in enumerate(paragraph_list):\n        new_paragraph = []\n        for sentence in paragraph:\n            sentence = ' '.join(sentence)\n            new_paragraph.append(sentence)\n        paragraph_dict[i] = new_paragraph\n\n    json_str = json.dumps(paragraph_dict)\n    num = len(paragraph_dict)\n    return json_str, num\ndef paragraph_to_text(raw_paragraph_list):\n    new_paragraph_list = []\n    for i, paragraph in enumerate(raw_paragraph_list):\n        paragraph_list = []\n        for sentence in paragraph:\n            sentence = ' '.join(sentence)\n            paragraph_list.append(sentence)\n        \n        new_paragraph = ' '.join(paragraph_list)\n        new_paragraph_list.append(new_paragraph)\n\n    paragraph_str = ' '.join(new_paragraph_list)\n    return paragraph_str\ndef summary_to_dict_str(summary_list):\n    summary_dict = {} # key = summary_id : value = summary sentence \n    for i, summary in enumerate(summary_list):\n        summary_dict[i] = ' '.join(summary)\n\n    json_str = json.dumps(summary_dict)\n    num = len(summary_dict)\n    return json_str, num\ndef summary_to_text(raw_summary_list):\n    summary_list = []\n    for i, summary in enumerate(raw_summary_list):\n        summary_list.append(' '.join(summary))\n\n    summary_str = ' '.join(summary_list)\n    return summary_str\ndef alter_json_data(json_list_data, filename=''):\n    new_json_list = []\n    for json_data in tqdm(json_list_data, desc=f'Altering json data {filename}'):\n        json_data = json_data.copy()\n        json_data['gold_labels'], _ = label_to_dict_str(json_data['gold_labels'])\n        json_data['news_text'] = paragraph_to_text(json_data['paragraphs'])\n        json_data['paragraphs'], num_paragraph = paragraph_to_dict_str(json_data['paragraphs'])\n        json_data['num_of_paragraphs'] = num_paragraph\n        json_data['summary_text'] = summary_to_text(json_data['summary'])\n        json_data['summary'], num_summary = summary_to_dict_str(json_data['summary'])\n        json_data['num_of_summary'] = num_summary\n        \n        new_json_list.append(json_data)\n    \n    return new_json_list\ndef create_dataset(jsonl):\n    header = list(jsonl[0].keys())\n    dataset_list = []\n    for json_data in jsonl:\n        row = []\n        for h in header:\n            row.append(json_data[h])\n        dataset_list.append(row)\n    \n    return header, dataset_list\ndef create_dataset_from_files(file_list):\n    df_header = None\n    dataset_list = []\n    for filename in file_list:\n        json_l = load_file_to_json_list(filename)\n        new_json_l = alter_json_data(json_l, filename)\n        header, dataset_part = create_dataset(new_json_l)\n        \n        if not df_header: df_header = header\n        dataset_list.extend(dataset_part)\n        \n    df_full = pd.DataFrame().from_records(dataset_list)\n    df_full = df_full.rename(columns=dict(enumerate(header)))\n    return df_full\ndf_train = create_dataset_from_files(train_files)\ndf_dev = create_dataset_from_files(dev_files)\ndf_test = create_dataset_from_files(test_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:51.751414Z","iopub.execute_input":"2025-04-06T07:49:51.751693Z","iopub.status.idle":"2025-04-06T07:49:57.048939Z","shell.execute_reply.started":"2025-04-06T07:49:51.751673Z","shell.execute_reply":"2025-04-06T07:49:57.048266Z"}},"outputs":[{"name":"stderr","text":"Loading data train.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14262/14262 [00:02<00:00, 6759.21it/s]\nAltering json data train.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14262/14262 [00:00<00:00, 14493.54it/s]\nLoading data dev.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 4215.42it/s]\nAltering json data dev.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 14699.80it/s]\nLoading data test.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3762/3762 [00:00<00:00, 10146.53it/s]\nAltering json data test.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3762/3762 [00:00<00:00, 14689.18it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Konversi DataFrame ke Dataset\ntrain_dataset = Dataset.from_pandas(df_train[['news_text', 'summary_text']])\ndev_dataset = Dataset.from_pandas(df_dev[['news_text', 'summary_text']])\ntest_dataset = Dataset.from_pandas(df_test[['news_text', 'summary_text']])\n\n# Gabungkan menjadi DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": dev_dataset,\n    \"test\": test_dataset\n})\n\n# Cek struktur dataset\nprint(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:57.050241Z","iopub.execute_input":"2025-04-06T07:49:57.050570Z","iopub.status.idle":"2025-04-06T07:49:57.527108Z","shell.execute_reply.started":"2025-04-06T07:49:57.050546Z","shell.execute_reply":"2025-04-06T07:49:57.526381Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['news_text', 'summary_text'],\n        num_rows: 14262\n    })\n    validation: Dataset({\n        features: ['news_text', 'summary_text'],\n        num_rows: 750\n    })\n    test: Dataset({\n        features: ['news_text', 'summary_text'],\n        num_rows: 3762\n    })\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df_train_h = df_train[df_train['category'] == 'hiburan']\ndf_dev_h = df_dev[df_dev['category'] == 'hiburan']\ndf_test_h = df_test[df_test['category'] == 'hiburan']\n\ntrain_dataset = Dataset.from_pandas(df_train_h[['news_text', 'summary_text']])\ndev_dataset = Dataset.from_pandas(df_dev_h[['news_text', 'summary_text']])\ntest_dataset = Dataset.from_pandas(df_test_h[['news_text', 'summary_text']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:57.527878Z","iopub.execute_input":"2025-04-06T07:49:57.528153Z","iopub.status.idle":"2025-04-06T07:49:57.593701Z","shell.execute_reply.started":"2025-04-06T07:49:57.528120Z","shell.execute_reply":"2025-04-06T07:49:57.592958Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n\n    # Other popular models!\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/Llama-3.3-70B\",\n    \"unsloth/mistral-7b-instruct-v0.3\",\n    \"unsloth/Phi-4\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3-1b-it-bnb-4bit\",\n    max_seq_length = 2048, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:49:57.594633Z","iopub.execute_input":"2025-04-06T07:49:57.594896Z","iopub.status.idle":"2025-04-06T07:50:49.171189Z","shell.execute_reply.started":"2025-04-06T07:49:57.594874Z","shell.execute_reply":"2025-04-06T07:50:49.170097Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nINFO 04-06 07:50:19 [__init__.py:239] Automatically detected platform cuda.\n==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/965M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d0633ac5cb472da79fc547d55efb59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f459483c8ec045f699ab968721cd447a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f066f12257534f139d910780b14e54a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6f0b50573a4f958f90158fb1956105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029efcbec8ba46489dd385f91599c541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030be4afd5f142b7a2eac491a2d0f049"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3840e97f3b0f4514ab7a7d4fff092cc6"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"\"\"\"# Akses embed_tokens dan lm_head langsung dari base_model\nmodel.base_model.model.embed_tokens = model.base_model.model.embed_tokens.to(torch.float32)\nmodel.lm_head = model.lm_head.to(torch.float32)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:49.172223Z","iopub.execute_input":"2025-04-06T07:50:49.172612Z","iopub.status.idle":"2025-04-06T07:50:49.177775Z","shell.execute_reply.started":"2025-04-06T07:50:49.172577Z","shell.execute_reply":"2025-04-06T07:50:49.176951Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'# Akses embed_tokens dan lm_head langsung dari base_model\\nmodel.base_model.model.embed_tokens = model.base_model.model.embed_tokens.to(torch.float32)\\nmodel.lm_head = model.lm_head.to(torch.float32)\\n'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_data_formats\nfrom unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:49.178783Z","iopub.execute_input":"2025-04-06T07:50:49.179127Z","iopub.status.idle":"2025-04-06T07:50:49.199950Z","shell.execute_reply.started":"2025-04-06T07:50:49.179095Z","shell.execute_reply":"2025-04-06T07:50:49.199083Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def convert_to_conversations(example):\n    return {\n        \"conversations\": [\n            {\n                \"role\": \"user\",\n                \"content\": f\"Ringkaskan teks berikut:\\n\\n{example['news_text']}\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": example[\"summary_text\"]\n            }\n        ]\n    }\n\n# Kalau train_data bertipe Dataset (bukan DatasetDict)\ntrain_dataset = train_dataset.map(convert_to_conversations)\ntest_dataset = test_dataset.map(convert_to_conversations)\ndev_dataset = dev_dataset.map(convert_to_conversations)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:49.201089Z","iopub.execute_input":"2025-04-06T07:50:49.201448Z","iopub.status.idle":"2025-04-06T07:50:49.484521Z","shell.execute_reply.started":"2025-04-06T07:50:49.201420Z","shell.execute_reply":"2025-04-06T07:50:49.483251Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1372 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703b7a8b27d2490fb52a28f50fe6fa7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f42d1250059942ffa5209563ea3026eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/76 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa8ccfed83b549048037524c3924aa4e"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Ambil hanya kolom 'conversations'\ntrain_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col != \"conversations\"])\ntest_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col != \"conversations\"])\ndev_dataset = dev_dataset.remove_columns([col for col in dev_dataset.column_names if col != \"conversations\"])\n\ntrain_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:49.487745Z","iopub.execute_input":"2025-04-06T07:50:49.488122Z","iopub.status.idle":"2025-04-06T07:50:49.505056Z","shell.execute_reply.started":"2025-04-06T07:50:49.488091Z","shell.execute_reply":"2025-04-06T07:50:49.503657Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'content': 'Ringkaskan teks berikut:\\n\\nJakarta , CNN Indonesia - - Dinas Pariwisata Provinsi Bengkulu kembali menggelar kegiatan Bimbingan Teknis ( Bimtek ) SDM Kepariwisataan dalam menyongson \" Visit 2020 Wonderful Bengkulu \" . Kegiatan yang berlangsung pada 8 hingga 10 November kemarin tersebut sebagai bagian dari upaya Pemerintah Provinsi Bengkulu dalam Hadir sebagai pemateri kegiatan pada 8 - 10 November itu adalah Plt. Asdep Strategi Pemasaran Pariwisata Nusantara , Deputi Bidang Pengembangan Pemasaran Pariwisata Nusantara Hariyanto serta perwakilan dari Deputi Bidang Pengembangan Kelembagaan Kementerian Pariwisata , Faizal . Kepala Dinas Pariwisata Provinsi Bengkulu Yudi Satria mengatakan , kegiatan Bimtek diikuti 250 peserta yang terdiri dari aparatur Pemerintah Provinsi , ASN Kabupaten / Kota , Kelompok Sadar Wisata serta pihak terkait sektor pariwisata di Bengkulu . \" Kegiatan ini dimaksudkan untuk memberikan pembekalan kepada peserta di bidang kepariwisataan , \" ujar Yudi Satria . Ia mengatakan , Pemprov telah menetapkan pariwisata sebagai salah satu sektor yang akan dikembangkan dan akan menjadi sektor unggulan dalam meningkatkan pertumbuhan ekonomi daerah serta masyarakat . Hal itu , jelas Yudi , tidak lepas dari potensi pariwisata di Bengkulu yang besar memiliki kekayaan alam yang indah serta budaya yang tinggi . \" Karena itu pula Pemprov telah menetapkan program \\' Visit 2020 Wonderful Bengkulu \" yang akan menjadi tujuan besar pariwisata Bengkulu . Salah satu poin utamanya adalah upaya menghasilkan SDM pariwisata yang andal yang akan diwujudkan melalui Bimtek ini , \" ujar Yudi . Selain itu , dalam menunjang proses \\' Visit 2020 Wonderful Bengkulu \\' , Pemprov juga telah menyiapkan 52 acara yang akan digelar dalam satu tahun ke depan yang bertujuan mengangkat potensi lokal ke kelas dunia . \" Salah satu yang dirancang secara besar adalah Sail Bengkulu yang akan menjual keindahan alam laut Bengkulu yang berhadapan langsung dengan Samudera Hindia , \" kata dia . Ajang lainnya tentunya Festival Tabot Muharam yang selalu menyedot minat ribuan wisatawan setiap tahun serta Festival Bumi Rafflesia . \" Semua kegiatan ini harus disinkronkan , penguatan SDM dan bagaimana promosi pemasarannya agar semua acara ini layak jual untuk wisatawan , \" ujarnya . Sementara Hariyanto dalam kesempatan itu menyampaikan materi tentang pengembangan SDM sektor pariwisata dan pengemasan serta pemasaran satu acara . Dalam paparanya ia menjelaskan , bahwa Bimtek penting untuk meningkatkan kompetensi SDM kepariwisataan sehingga mampu berperan dalam peningkatan pembangunan kepariwisataan . Selain itu ia menegaskan bahwa dalam pengembangan pariwisata , promosi juga hal yang harus diperhatikan . Misalnya penekanan pada mekanisme promosi pariwisata secara digital atau daring , penyelenggaraan acara juga harus terkurasi dengan baik sehingga memiliki daya tarik yang kuat . \" Bagaimana juga meningkatkan efektivitas partisipasi Dinas Pariwisata pada penyelenggaraan acara dan bagaimana cara memasarkan dan mempromosikan agar bisa dikenal dan menjadi daya tarik wisatawan , â€ terang Hariyanto . Deputi Pengembangan Pemasaran Pariwisata Nusantara , Esthy Reko Astuti mengatakan , Bimtek kali ini juga bertujuan memberi perspektif dan arah yang sama tentang program promosi Pariwisata di Bengkulu . \" Selain itu juga untuk memahami potensi destinasi - destinasi wisata di Bengkulu , \" ujar Esthy . Menteri Pariwisata Arief Yahya mengapresiasi kegiatan Bimtek sebagai salah satu upaya dan komitmen dari Pemerintah Provinsi Bengkulu dalam mewujudkan pariwisata sebagai salah satu sektor utama . Ia pun berkomitmen akan mendung Penprov dalam mewujudkan \" Visit 2020 Wonderful Bengkulu \" . \" Obyek wisata andalan Bengkulu , Benteng Marlborough , Rumah Bung Karno , Pantai Panjang yang memiliki pasir putih yang indah dan bersih . Semuanya world class , ditambah dengan puluhan atraksi didalamnya , tinggal kita akan dukung dan promosikan \\' Bengkulu Visit \\' sehingga lebih mendunia , \" kata Arief ( syahb )',\n   'role': 'user'},\n  {'content': 'Dinas Pariwisata Provinsi Bengkulu kembali menggelar kegiatan Bimbingan Teknis ( Bimtek ) SDM Kepariwisataan dalam menyongson \" Visit 2020 Wonderful Bengkulu \" pada 8 - 10 November 2017 yang lalu . Kegiatan yang berlangsung pada 8 hingga 10 November kemarin tersebut sebagai bagian dari upaya Pemerintah Provinsi Bengkulu dalam memperkuat SDM Pariwisata untuk menyongsong \" Visit 2020 Wonderful Bengkulu \" .',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def apply_chat_template(examples):\n    texts = tokenizer.apply_chat_template(examples[\"conversations\"], tokenize=False)\n    return { \"text\" : texts }\npass\ntrain_dataset = train_dataset.map(apply_chat_template, batched = True)\ntest_dataset = test_dataset.map(apply_chat_template, batched = True)\ndev_dataset = dev_dataset.map(apply_chat_template, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:49.507251Z","iopub.execute_input":"2025-04-06T07:50:49.507628Z","iopub.status.idle":"2025-04-06T07:50:51.297732Z","shell.execute_reply.started":"2025-04-06T07:50:49.507599Z","shell.execute_reply":"2025-04-06T07:50:51.296731Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1372 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e648021861914d85b15cfd79e573212b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b7c07c7fc54fabb158e49d63e29606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/76 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343c53864a0d412885b3c32890b0bb33"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers     = False, # Turn off for just text!\n    finetune_language_layers   = True,  # Should leave on!\n    finetune_attention_modules = True,  # Attention good for GRPO\n    finetune_mlp_modules       = True,  # SHould leave on always!\n\n    r = 8,           # Larger = higher accuracy, but might overfit\n    lora_alpha = 8,  # Recommended alpha == r at least\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:51.298911Z","iopub.execute_input":"2025-04-06T07:50:51.299264Z","iopub.status.idle":"2025-04-06T07:50:56.756558Z","shell.execute_reply.started":"2025-04-06T07:50:51.299238Z","shell.execute_reply":"2025-04-06T07:50:56.755537Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model` require gradients\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\"\"\"!pip install evaluate\n!pip install rouge_score\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:56.757596Z","iopub.execute_input":"2025-04-06T07:50:56.757930Z","iopub.status.idle":"2025-04-06T07:50:56.764663Z","shell.execute_reply.started":"2025-04-06T07:50:56.757899Z","shell.execute_reply":"2025-04-06T07:50:56.763385Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'!pip install evaluate\\n!pip install rouge_score\\n'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"\"\"\"import evaluate\nimport numpy as np\n\nrouge = evaluate.load(\"rouge\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:56.765897Z","iopub.execute_input":"2025-04-06T07:50:56.766266Z","iopub.status.idle":"2025-04-06T07:50:56.896979Z","shell.execute_reply.started":"2025-04-06T07:50:56.766232Z","shell.execute_reply":"2025-04-06T07:50:56.895976Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'import evaluate\\nimport numpy as np\\n\\nrouge = evaluate.load(\"rouge\")\\n'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"\"\"\"def compute_metrics(eval_preds):\n    predictions, labels = eval_preds\n\n    # ROUGE hanya menerima teks, jadi perlu decoding\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # Kadang label pakai -100 untuk padding, ubah dulu ke tokenizer.pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Bersihkan whitespace\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n\n    # Hitung ROUGE\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    \n    return {\n        \"rouge1\": result[\"rouge1\"],\n        \"rouge2\": result[\"rouge2\"],\n        \"rougeL\": result[\"rougeL\"],\n        \"rougeLsum\": result[\"rougeLsum\"],\n    }\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:56.898026Z","iopub.execute_input":"2025-04-06T07:50:56.898422Z","iopub.status.idle":"2025-04-06T07:50:56.911479Z","shell.execute_reply.started":"2025-04-06T07:50:56.898383Z","shell.execute_reply":"2025-04-06T07:50:56.910545Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'def compute_metrics(eval_preds):\\n    predictions, labels = eval_preds\\n\\n    # ROUGE hanya menerima teks, jadi perlu decoding\\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\\n    \\n    # Kadang label pakai -100 untuk padding, ubah dulu ke tokenizer.pad_token_id\\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\\n\\n    # Bersihkan whitespace\\n    decoded_preds = [pred.strip() for pred in decoded_preds]\\n    decoded_labels = [label.strip() for label in decoded_labels]\\n\\n    # Hitung ROUGE\\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\\n    \\n    return {\\n        \"rouge1\": result[\"rouge1\"],\\n        \"rouge2\": result[\"rouge2\"],\\n        \"rougeL\": result[\"rougeL\"],\\n        \"rougeLsum\": result[\"rougeLsum\"],\\n    }\\n'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import wandb\nwandb.login(key='ea5b934c345990bf66ca82b76040cf0748acdb7a')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:50:56.912464Z","iopub.execute_input":"2025-04-06T07:50:56.912800Z","iopub.status.idle":"2025-04-06T07:51:05.839523Z","shell.execute_reply.started":"2025-04-06T07:50:56.912769Z","shell.execute_reply":"2025-04-06T07:51:05.838513Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjauharulumam\u001b[0m (\u001b[33mjauharulumam-uin-walisongo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = dev_dataset, # Can set up evaluation!\n    args = SFTConfig(\n        evaluation_strategy=\"steps\",   # â† aktifkan evaluasi\n        eval_steps=10,                  # â† setiap berapa langkah mau evaluasi\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n        warmup_steps = 5,\n        num_train_epochs = 3, # Set this for 1 full training run.\n        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n        logging_steps = 5,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"wandb\",\n        logging_dir = \"./logs\",\n        output_dir = \"./results\",  # direktori checkpoint\n        save_strategy = \"epoch\",  # Simpan tiap epoch\n        save_total_limit = 3,# Use this for WandB etc\n        save_safetensors=True,\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:05.840694Z","iopub.execute_input":"2025-04-06T07:51:05.841389Z","iopub.status.idle":"2025-04-06T07:51:20.836727Z","shell.execute_reply.started":"2025-04-06T07:51:05.841344Z","shell.execute_reply":"2025-04-06T07:51:20.835532Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/1372 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f2c361a2e74df0a5a98d874397b51d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/76 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"212c75f1549147ad993952965e0ab533"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"\"\"\"from transformers import TrainerCallback\nimport wandb\n\nclass WandbModelCheckpoint(TrainerCallback):\n    def on_save(self, args, state, control, **kwargs):\n        artifact = wandb.Artifact(f\"model-epoch-{state.epoch:.0f}\", type=\"model\")\n        artifact.add_dir(args.output_dir)\n        wandb.log_artifact(artifact)\n\ntrainer.add_callback(WandbModelCheckpoint())\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:20.837996Z","iopub.execute_input":"2025-04-06T07:51:20.838309Z","iopub.status.idle":"2025-04-06T07:51:20.844927Z","shell.execute_reply.started":"2025-04-06T07:51:20.838281Z","shell.execute_reply":"2025-04-06T07:51:20.844145Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'from transformers import TrainerCallback\\nimport wandb\\n\\nclass WandbModelCheckpoint(TrainerCallback):\\n    def on_save(self, args, state, control, **kwargs):\\n        artifact = wandb.Artifact(f\"model-epoch-{state.epoch:.0f}\", type=\"model\")\\n        artifact.add_dir(args.output_dir)\\n        wandb.log_artifact(artifact)\\n\\ntrainer.add_callback(WandbModelCheckpoint())\\n'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<start_of_turn>user\\n\",\n    response_part = \"<start_of_turn>model\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:20.845974Z","iopub.execute_input":"2025-04-06T07:51:20.846352Z","iopub.status.idle":"2025-04-06T07:51:23.043979Z","shell.execute_reply.started":"2025-04-06T07:51:20.846315Z","shell.execute_reply":"2025-04-06T07:51:23.042690Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1372 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7849f23f70d4c919fb1d082143c606b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/76 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d5de70eac142c482d1b4343befcfb8"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:23.045394Z","iopub.execute_input":"2025-04-06T07:51:23.045890Z","iopub.status.idle":"2025-04-06T07:51:23.062425Z","shell.execute_reply.started":"2025-04-06T07:51:23.045848Z","shell.execute_reply":"2025-04-06T07:51:23.061239Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'<bos><bos><start_of_turn>user\\nRingkaskan teks berikut:\\n\\nMerdeka.com - Thilafushi , mungkin nama ini masih asing terdengar . Thilafushi adalah salah satu dari gugusan pulau-pulau bagian dari wilayah negeri Maladewa . Seperti telah banyak diketahui , Maladewa terkenal dengan industri pariwisatanya . Negeri ini memiliki sejumlah pantai tropis dengan air lair berwarna kehijauan , pastinya indah . Namun Thilafushi menampilkan kenyataan yang jauh berbeda dengan pulau-pulau lainnya di Maladewa . Thilafushi adalah pulau buatan hasil reklamasi . Jika di tempat lain terdapat pantai-pantai yang indah , pulau ini merupakan pusat pembuangan limbah . Bahkan pulau ini pun dibuat dari tumpukan sampah . Dulunya Thilafushi merupakan sebuah laguna . Kemajuan industri pariwisata memaksa pemerintah Maladewa untuk mereklamasi laguna ini , karena kebutuhan akan tempat pembuangan semakin mendesak . Berawal pada Desember 1991 penggalian lubang penampungan limbah mulai dilakukan . Sampah - sampah berdatangan dari seluruh penjuru Maldives . Diendapkan ke dalam lubang berukuran 1060 meter kubik hingga penuh . Bagian atasnya ditutup dengan puing-puing bangunan , merata dengan ketinggian tanah di sekitarnya . Terakhir , kemudian bagian permukaannya ditutup dengan pasir pantai . Proyek reklamasi tersebut berjalan dengan baik sampai sekarang . Bahkan sebagian wilayahnya kini menjadi daerah industri dan pemukiman . Saat ini , setidaknya ada lebih dari 30 pabrik berdiri di Pulau Thilafushi . Mulai dari pabrik pengemasan semen dan gas , manufaktur perahu , hingga pergudangan . Pulau ini juga menjadi tempat tinggal sekitar 150 imigran asal Bangladesh yang sehari-hari bekerja memilah sampah . Pulau Sampah Thilafushi , Maladewa 2014 Merdeka.com / Populer Mechanics Reklamasi Thilafushi pun masih berjalan sampai sekarang dengan sampah - sampah yang terus berdatangan . Membuat luas pulau pembuangan ini bertambah 1 meter persegi setiap harinya . Sayangnya , belakangan sampah - sampah di pulau ini terhanyut diterjang ombak . Mengotori laut dan mencemari keindahan spot - spot diving di pulau wisata sekitar . Meski reklamasi sempat dihentikan , pemerintah Maldives melanjutkannya kembali . Sebagian jenis sampah kini diekspor ke India untuk didaur ulang . [ gni ]<end_of_turn>\\n<start_of_turn>model\\nThilafushi adalah salah satu dari gugusan pulau-pulau bagian dari wilayah negeri Maladewa . Namun Thilafushi menampilkan kenyataan yang jauh berbeda dengan pulau-pulau lainnya di Maladewa . Thilafushi adalah pulau buatan hasil reklamasi . Jika di tempat lain terdapat pantai-pantai yang indah , pulau ini merupakan pusat pembuangan limbah . Bahkan pulau ini pun dibuat dari tumpukan sampah .<end_of_turn>\\n'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:23.063578Z","iopub.execute_input":"2025-04-06T07:51:23.063889Z","iopub.status.idle":"2025-04-06T07:51:23.089782Z","shell.execute_reply.started":"2025-04-06T07:51:23.063865Z","shell.execute_reply":"2025-04-06T07:51:23.088626Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Thilafushi adalah salah satu dari gugusan pulau-pulau bagian dari wilayah negeri Maladewa . Namun Thilafushi menampilkan kenyataan yang jauh berbeda dengan pulau-pulau lainnya di Maladewa . Thilafushi adalah pulau buatan hasil reklamasi . Jika di tempat lain terdapat pantai-pantai yang indah , pulau ini merupakan pusat pembuangan limbah . Bahkan pulau ini pun dibuat dari tumpukan sampah .<end_of_turn>\\n'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:23.090906Z","iopub.execute_input":"2025-04-06T07:51:23.091343Z","iopub.status.idle":"2025-04-06T07:51:23.106199Z","shell.execute_reply.started":"2025-04-06T07:51:23.091300Z","shell.execute_reply":"2025-04-06T07:51:23.105022Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n1.473 GB of memory reserved.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:51:23.107506Z","iopub.execute_input":"2025-04-06T07:51:23.107953Z","iopub.status.idle":"2025-04-06T08:33:35.216565Z","shell.execute_reply.started":"2025-04-06T07:51:23.107913Z","shell.execute_reply":"2025-04-06T08:33:35.215510Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,372 | Num Epochs = 3 | Total steps = 255\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 6,522,880/1,000,000,000 (0.65% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250406_075125-3a36gy1h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jauharulumam-uin-walisongo/huggingface/runs/3a36gy1h' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/jauharulumam-uin-walisongo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jauharulumam-uin-walisongo/huggingface' target=\"_blank\">https://wandb.ai/jauharulumam-uin-walisongo/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jauharulumam-uin-walisongo/huggingface/runs/3a36gy1h' target=\"_blank\">https://wandb.ai/jauharulumam-uin-walisongo/huggingface/runs/3a36gy1h</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [255/255 41:09, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.074000</td>\n      <td>0.864247</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.736800</td>\n      <td>0.761549</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.703900</td>\n      <td>0.723947</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.582000</td>\n      <td>0.706793</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.636700</td>\n      <td>0.694774</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.675400</td>\n      <td>0.687771</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.605900</td>\n      <td>0.685369</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.757900</td>\n      <td>0.684370</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.543700</td>\n      <td>0.689673</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.547100</td>\n      <td>0.705891</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.480600</td>\n      <td>0.698568</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.587600</td>\n      <td>0.694052</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.557500</td>\n      <td>0.688055</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.616500</td>\n      <td>0.685282</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.592400</td>\n      <td>0.676350</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.582200</td>\n      <td>0.676693</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.640600</td>\n      <td>0.684331</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.556300</td>\n      <td>0.690747</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.459300</td>\n      <td>0.698702</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.472400</td>\n      <td>0.701650</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.512000</td>\n      <td>0.698901</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.482800</td>\n      <td>0.699732</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.455000</td>\n      <td>0.698252</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.379500</td>\n      <td>0.698387</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.390500</td>\n      <td>0.698496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but Gemma3ForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:33:35.217759Z","iopub.execute_input":"2025-04-06T08:33:35.218829Z","iopub.status.idle":"2025-04-06T08:33:35.231488Z","shell.execute_reply.started":"2025-04-06T08:33:35.218800Z","shell.execute_reply":"2025-04-06T08:33:35.230606Z"}},"outputs":[{"name":"stdout","text":"2529.5644 seconds used for training.\n42.16 minutes used for training.\nPeak reserved memory = 2.238 GB.\nPeak reserved memory for training = 0.765 GB.\nPeak reserved memory % of max memory = 15.182 %.\nPeak reserved memory for training % of max memory = 5.19 %.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [{\n        \"type\" : \"text\",\n        \"text\" : \"Ringkaskan teks berikut:\\n\\n Thilafushi , mungkin nama ini masih asing terdengar . Thilafushi adalah salah satu dari gugusan pulau-pulau bagian dari wilayah negeri Maladewa . Seperti telah banyak diketahui , Maladewa terkenal dengan industri pariwisatanya . Negeri ini memiliki sejumlah pantai tropis dengan air lair berwarna kehijauan , pastinya indah . Namun Thilafushi menampilkan kenyataan yang jauh berbeda dengan pulau-pulau lainnya di Maladewa . Thilafushi adalah pulau buatan hasil reklamasi . Jika di tempat lain terdapat pantai-pantai yang indah , pulau ini merupakan pusat pembuangan limbah . Bahkan pulau ini pun dibuat dari tumpukan sampah . Dulunya Thilafushi merupakan sebuah laguna . Kemajuan industri pariwisata memaksa pemerintah Maladewa untuk mereklamasi laguna ini , karena kebutuhan akan tempat pembuangan semakin mendesak . Berawal pada Desember 1991 penggalian lubang penampungan limbah mulai dilakukan . Sampah - sampah berdatangan dari seluruh penjuru Maldives . Diendapkan ke dalam lubang berukuran 1060 meter kubik hingga penuh . Bagian atasnya ditutup dengan puing-puing bangunan , merata dengan ketinggian tanah di sekitarnya . Terakhir , kemudian bagian permukaannya ditutup dengan pasir pantai . Proyek reklamasi tersebut berjalan dengan baik sampai sekarang . Bahkan sebagian wilayahnya kini menjadi daerah industri dan pemukiman . Saat ini , setidaknya ada lebih dari 30 pabrik berdiri di Pulau Thilafushi . Mulai dari pabrik pengemasan semen dan gas , manufaktur perahu , hingga pergudangan . Pulau ini juga menjadi tempat tinggal sekitar 150 imigran asal Bangladesh yang sehari-hari bekerja memilah sampah . Pulau Sampah Thilafushi , Maladewa 2014 Merdeka.com / Populer Mechanics Reklamasi Thilafushi pun masih berjalan sampai sekarang dengan sampah - sampah yang terus berdatangan . Membuat luas pulau pembuangan ini bertambah 1 meter persegi setiap harinya . Sayangnya , belakangan sampah - sampah di pulau ini terhanyut diterjang ombak . Mengotori laut dan mencemari keindahan spot - spot diving di pulau wisata sekitar . Meski reklamasi sempat dihentikan , pemerintah Maldives melanjutkannya kembali . Sebagian jenis sampah kini diekspor ke India untuk didaur ulang .\",\n    }]\n}]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt = True, # Must add for generation\n)\noutputs = model.generate(\n    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 128, # Increase for longer outputs!\n    # Recommended Gemma-3 settings!\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:33:35.232394Z","iopub.execute_input":"2025-04-06T08:33:35.232629Z","iopub.status.idle":"2025-04-06T08:33:59.380667Z","shell.execute_reply.started":"2025-04-06T08:33:35.232610Z","shell.execute_reply":"2025-04-06T08:33:59.379835Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['<bos><bos><start_of_turn>user\\nRingkaskan teks berikut:\\n\\n Thilafushi , mungkin nama ini masih asing terdengar . Thilafushi adalah salah satu dari gugusan pulau-pulau bagian dari wilayah negeri Maladewa . Seperti telah banyak diketahui , Maladewa terkenal dengan industri pariwisatanya . Negeri ini memiliki sejumlah pantai tropis dengan air lair berwarna kehijauan , pastinya indah . Namun Thilafushi menampilkan kenyataan yang jauh berbeda dengan pulau-pulau lainnya di Maladewa . Thilafushi adalah pulau buatan hasil reklamasi . Jika di tempat lain terdapat pantai-pantai yang indah , pulau ini merupakan pusat pembuangan limbah . Bahkan pulau ini pun dibuat dari tumpukan sampah . Dulunya Thilafushi merupakan sebuah laguna . Kemajuan industri pariwisata memaksa pemerintah Maladewa untuk mereklamasi laguna ini , karena kebutuhan akan tempat pembuangan semakin mendesak . Berawal pada Desember 1991 penggalian lubang penampungan limbah mulai dilakukan . Sampah - sampah berdatangan dari seluruh penjuru Maldives . Diendapkan ke dalam lubang berukuran 1060 meter kubik hingga penuh . Bagian atasnya ditutup dengan puing-puing bangunan , merata dengan ketinggian tanah di sekitarnya . Terakhir , kemudian bagian permukaannya ditutup dengan pasir pantai . Proyek reklamasi tersebut berjalan dengan baik sampai sekarang . Bahkan sebagian wilayahnya kini menjadi daerah industri dan pemukiman . Saat ini , setidaknya ada lebih dari 30 pabrik berdiri di Pulau Thilafushi . Mulai dari pabrik pengemasan semen dan gas , manufaktur perahu , hingga pergudangan . Pulau ini juga menjadi tempat tinggal sekitar 150 imigran asal Bangladesh yang sehari-hari bekerja memilah sampah . Pulau Sampah Thilafushi , Maladewa 2014 Merdeka.com / Populer Mechanics Reklamasi Thilafushi pun masih berjalan sampai sekarang dengan sampah - sampah yang terus berdatangan . Membuat luas pulau pembuangan ini bertambah 1 meter persegi setiap harinya . Sayangnya , belakangan sampah - sampah di pulau ini terhanyut diterjang ombak . Mengotori laut dan mencemari keindahan spot - spot diving di pulau wisata sekitar . Meski reklamasi sempat dihentikan , pemerintah Maldives melanjutkannya kembali . Sebagian jenis sampah kini diekspor ke India untuk didaur ulang .<end_of_turn>\\n<start_of_turn>model\\nThilafushi adalah salah satu dari gugusan pulau-pulau bagian dari wilayah negeri Maladewa . Seperti telah banyak diketahui , Maladewa terkenal dengan industri pariwisatanya . Negeri ini memiliki sejumlah pantai tropis dengan air lair berwarna kehijauan , pastinya indah . Namun Thilafushi menampilkan kenyataan yang jauh berbeda dengan pulau-pulau lainnya di Maladewa . Thilafushi adalah pulau buatan hasil reklamasi .<end_of_turn>']"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"!zip -r /kaggle/working/my_model_checkpoint_86.zip /kaggle/working/results/checkpoint-86\n!zip -r /kaggle/working/my_model_checkpoint_172.zip /kaggle/working/results/checkpoint-172\n!zip -r /kaggle/working/my_model_checkpoint_255.zip /kaggle/working/results/checkpoint-255","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:39:06.848394Z","iopub.execute_input":"2025-04-06T08:39:06.848819Z","iopub.status.idle":"2025-04-06T08:39:17.523896Z","shell.execute_reply.started":"2025-04-06T08:39:06.848791Z","shell.execute_reply":"2025-04-06T08:39:17.522925Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/results/checkpoint-86/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-86/trainer_state.json (deflated 76%)\n  adding: kaggle/working/results/checkpoint-86/tokenizer.model (deflated 52%)\n  adding: kaggle/working/results/checkpoint-86/rng_state.pth (deflated 25%)\n  adding: kaggle/working/results/checkpoint-86/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/results/checkpoint-86/README.md (deflated 66%)\n  adding: kaggle/working/results/checkpoint-86/tokenizer_config.json (deflated 96%)\n  adding: kaggle/working/results/checkpoint-86/optimizer.pt (deflated 10%)\n  adding: kaggle/working/results/checkpoint-86/scheduler.pt (deflated 56%)\n  adding: kaggle/working/results/checkpoint-86/tokenizer.json (deflated 83%)\n  adding: kaggle/working/results/checkpoint-86/adapter_config.json (deflated 56%)\n  adding: kaggle/working/results/checkpoint-86/added_tokens.json (stored 0%)\n  adding: kaggle/working/results/checkpoint-86/special_tokens_map.json (deflated 77%)\n  adding: kaggle/working/results/checkpoint-86/training_args.bin (deflated 51%)\n  adding: kaggle/working/results/checkpoint-172/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-172/trainer_state.json (deflated 80%)\n  adding: kaggle/working/results/checkpoint-172/tokenizer.model (deflated 52%)\n  adding: kaggle/working/results/checkpoint-172/rng_state.pth (deflated 25%)\n  adding: kaggle/working/results/checkpoint-172/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/results/checkpoint-172/README.md (deflated 66%)\n  adding: kaggle/working/results/checkpoint-172/tokenizer_config.json (deflated 96%)\n  adding: kaggle/working/results/checkpoint-172/optimizer.pt (deflated 10%)\n  adding: kaggle/working/results/checkpoint-172/scheduler.pt (deflated 56%)\n  adding: kaggle/working/results/checkpoint-172/tokenizer.json (deflated 83%)\n  adding: kaggle/working/results/checkpoint-172/adapter_config.json (deflated 56%)\n  adding: kaggle/working/results/checkpoint-172/added_tokens.json (stored 0%)\n  adding: kaggle/working/results/checkpoint-172/special_tokens_map.json (deflated 77%)\n  adding: kaggle/working/results/checkpoint-172/training_args.bin (deflated 51%)\n  adding: kaggle/working/results/checkpoint-255/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-255/trainer_state.json (deflated 81%)\n  adding: kaggle/working/results/checkpoint-255/tokenizer.model (deflated 52%)\n  adding: kaggle/working/results/checkpoint-255/rng_state.pth (deflated 25%)\n  adding: kaggle/working/results/checkpoint-255/adapter_model.safetensors (deflated 7%)\n  adding: kaggle/working/results/checkpoint-255/README.md (deflated 66%)\n  adding: kaggle/working/results/checkpoint-255/tokenizer_config.json (deflated 96%)\n  adding: kaggle/working/results/checkpoint-255/optimizer.pt (deflated 10%)\n  adding: kaggle/working/results/checkpoint-255/scheduler.pt (deflated 56%)\n  adding: kaggle/working/results/checkpoint-255/tokenizer.json (deflated 83%)\n  adding: kaggle/working/results/checkpoint-255/adapter_config.json (deflated 56%)\n  adding: kaggle/working/results/checkpoint-255/added_tokens.json (stored 0%)\n  adding: kaggle/working/results/checkpoint-255/special_tokens_map.json (deflated 77%)\n  adding: kaggle/working/results/checkpoint-255/training_args.bin (deflated 51%)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}