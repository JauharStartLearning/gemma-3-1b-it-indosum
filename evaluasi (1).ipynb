{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5167973,"sourceType":"datasetVersion","datasetId":3003478},{"sourceId":11299765,"sourceType":"datasetVersion","datasetId":7066309},{"sourceId":11300477,"sourceType":"datasetVersion","datasetId":7066878}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install seacrowd accelerate peft bitsandbytes wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:08:27.903112Z","iopub.execute_input":"2025-04-07T09:08:27.903296Z","iopub.status.idle":"2025-04-07T09:08:52.100069Z","shell.execute_reply.started":"2025-04-07T09:08:27.903278Z","shell.execute_reply":"2025-04-07T09:08:52.099106Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nCollecting seacrowd\n  Downloading seacrowd-0.2.2-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nCollecting loguru>=0.5.3 (from seacrowd)\n  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\nCollecting bioc>=1.3.7 (from seacrowd)\n  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: pandas>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (2.2.3)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (1.26.4)\nRequirement already satisfied: datasets>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.3.1)\nCollecting black~=22.0 (from seacrowd)\n  Downloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (52 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting flake8>=3.8.3 (from seacrowd)\n  Downloading flake8-7.2.0-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting isort>=5.0.0 (from seacrowd)\n  Downloading isort-6.0.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.11.12)\nCollecting pre-commit>=2.19.0 (from seacrowd)\n  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting jsonlines>=3.1.0 (from seacrowd)\n  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: torchaudio>=0.11 in /usr/local/lib/python3.10/dist-packages (from seacrowd) (2.5.1+cu121)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from seacrowd) (0.12.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from seacrowd) (0.10.2.post1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.2.4)\nCollecting zstandard (from seacrowd)\n  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting ffmpeg (from seacrowd)\n  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting conllu (from seacrowd)\n  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from seacrowd) (3.1.5)\nCollecting translate-toolkit>=3.7.3 (from seacrowd)\n  Downloading translate_toolkit-3.15.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from seacrowd) (4.12.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->seacrowd) (1.18.3)\nRequirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.10/dist-packages (from bioc>=1.3.7->seacrowd) (5.3.0)\nCollecting intervaltree (from bioc>=1.3.7->seacrowd)\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting docopt (from bioc>=1.3.7->seacrowd)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black~=22.0->seacrowd) (1.0.0)\nCollecting pathspec>=0.9.0 (from black~=22.0->seacrowd)\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black~=22.0->seacrowd) (2.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.2.0->seacrowd) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.2.0->seacrowd) (2024.12.0)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nCollecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.8.3->seacrowd)\n  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\nCollecting pycodestyle<2.14.0,>=2.13.0 (from flake8>=3.8.3->seacrowd)\n  Downloading pycodestyle-2.13.0-py2.py3-none-any.whl.metadata (4.5 kB)\nCollecting pyflakes<3.4.0,>=3.3.0 (from flake8>=3.8.3->seacrowd)\n  Downloading pyflakes-3.3.2-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->seacrowd) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.3->seacrowd) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.3->seacrowd) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.3->seacrowd) (2025.1)\nCollecting cfgv>=2.0.0 (from pre-commit>=2.19.0->seacrowd)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify>=1.0.0 (from pre-commit>=2.19.0->seacrowd)\n  Downloading identify-2.6.9-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv>=0.11.1 (from pre-commit>=2.19.0->seacrowd)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nCollecting virtualenv>=20.10.0 (from pre-commit>=2.19.0->seacrowd)\n  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\nCollecting cwcwidth<0.2,>=0.1.10 (from translate-toolkit>=3.7.3->seacrowd)\n  Downloading cwcwidth-0.1.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (0.60.0)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->seacrowd) (1.1.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->seacrowd) (1.17.1)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->seacrowd) (2.0.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->seacrowd) (2.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->seacrowd) (0.43.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->seacrowd) (3.5.0)\nCollecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=2.19.0->seacrowd)\n  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree->bioc>=1.3.7->seacrowd) (2.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20->seacrowd) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20->seacrowd) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->seacrowd) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20->seacrowd) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20->seacrowd) (2024.2.0)\nDownloading seacrowd-0.2.2-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\nDownloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading flake8-7.2.0-py2.py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading isort-6.0.1-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\nDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading translate_toolkit-3.15.1-py3-none-any.whl (744 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m744.9/744.9 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading conllu-6.0.0-py3-none-any.whl (16 kB)\nDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading cwcwidth-0.1.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading identify-2.6.9-py2.py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\nDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading pycodestyle-2.13.0-py2.py3-none-any.whl (31 kB)\nDownloading pyflakes-3.3.2-py2.py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ffmpeg, docopt, intervaltree\n  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=413553d1e1a294ccaff6410b651a1fb55c7d0533baa7f8c5cf2a979e6867ab57\n  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a70dcc41e87952bdf249fd0effe0779e0e3b511c2a793e05a53c5275725b5465\n  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=1bd8a4f830a3594fb4f92ff9146f247e4fbe8c0409b9ca3e40f895eec12ea930\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\nSuccessfully built ffmpeg docopt intervaltree\nInstalling collected packages: ffmpeg, docopt, distlib, zstandard, virtualenv, pyflakes, pycodestyle, pathspec, nodeenv, mccabe, loguru, jsonlines, isort, intervaltree, identify, cwcwidth, conllu, cfgv, translate-toolkit, pre-commit, flake8, black, bioc, seacrowd, bitsandbytes\nSuccessfully installed bioc-2.1 bitsandbytes-0.45.4 black-22.12.0 cfgv-3.4.0 conllu-6.0.0 cwcwidth-0.1.10 distlib-0.3.9 docopt-0.6.2 ffmpeg-1.4 flake8-7.2.0 identify-2.6.9 intervaltree-3.1.0 isort-6.0.1 jsonlines-4.0.0 loguru-0.7.3 mccabe-0.7.0 nodeenv-1.9.1 pathspec-0.12.1 pre-commit-4.2.0 pycodestyle-2.13.0 pyflakes-3.3.2 seacrowd-0.2.2 translate-toolkit-3.15.1 virtualenv-20.30.0 zstandard-0.23.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install unsloth vllm\n!pip install triton==3.1.0\n!pip install -U pynvml\n# Install latest Hugging Face for Gemma-3!\n!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:08:52.100985Z","iopub.execute_input":"2025-04-07T09:08:52.101261Z","iopub.status.idle":"2025-04-07T09:14:32.093674Z","shell.execute_reply.started":"2025-04-07T09:08:52.101238Z","shell.execute_reply":"2025-04-07T09:14:32.092440Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random \nimport torch\nimport datasets\n\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:15:14.902072Z","iopub.execute_input":"2025-04-07T09:15:14.902392Z","iopub.status.idle":"2025-04-07T09:15:20.678347Z","shell.execute_reply.started":"2025-04-07T09:15:14.902366Z","shell.execute_reply":"2025-04-07T09:15:20.677456Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Detect train, dev, and test files\nDATASET_ROOT = '/kaggle/input/indosum/indosum'\n\nfiles_id_dir = os.listdir(DATASET_ROOT)\ntrain_files = []\ndev_files = []\ntest_files = []\n\nfor filename in files_id_dir:\n    if 'train' in filename:\n        train_files.append(filename)\n    elif 'dev' in filename:\n        dev_files.append(filename)\n    elif 'test' in filename:\n        test_files.append(filename)\n\ntrain_files, dev_files, test_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:15:20.679532Z","iopub.execute_input":"2025-04-07T09:15:20.679947Z","iopub.status.idle":"2025-04-07T09:15:20.733588Z","shell.execute_reply.started":"2025-04-07T09:15:20.679924Z","shell.execute_reply":"2025-04-07T09:15:20.732804Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(['train.01.jsonl',\n  'train.05.jsonl',\n  'train.03.jsonl',\n  'train.04.jsonl',\n  'train.02.jsonl'],\n ['dev.01.jsonl',\n  'dev.05.jsonl',\n  'dev.04.jsonl',\n  'dev.03.jsonl',\n  'dev.02.jsonl'],\n ['test.05.jsonl',\n  'test.04.jsonl',\n  'test.02.jsonl',\n  'test.03.jsonl',\n  'test.01.jsonl'])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_files = ['train.01.jsonl']\ntest_files = ['test.01.jsonl']\ndev_files = ['dev.01.jsonl']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:15:20.735057Z","iopub.execute_input":"2025-04-07T09:15:20.735298Z","iopub.status.idle":"2025-04-07T09:15:20.738325Z","shell.execute_reply.started":"2025-04-07T09:15:20.735279Z","shell.execute_reply":"2025-04-07T09:15:20.737525Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_file_to_json_list(filename):\n    file = os.path.join(DATASET_ROOT, filename)\n    data = []\n    with open(file, 'r') as f:\n        json_list = list(f)\n        for json_str in tqdm(json_list, desc=f'Loading data {filename}'):\n            d = json.loads(json_str)\n            data.append(d)\n    return data\n\ndef label_to_dict_str(label_list):\n    label_dict = {} # key = paragraph_id : value = label list \n    for i, label in enumerate(label_list[:]):\n        label_dict[i] = label\n\n    json_str = json.dumps(label_dict)\n    num = len(label_dict)\n    return json_str, num\n\ndef paragraph_to_dict_str(paragraph_list):\n    paragraph_dict = {} # key = paragraph_id : value = paragraph list \n    for i, paragraph in enumerate(paragraph_list):\n        new_paragraph = []\n        for sentence in paragraph:\n            sentence = ' '.join(sentence)\n            new_paragraph.append(sentence)\n        paragraph_dict[i] = new_paragraph\n\n    json_str = json.dumps(paragraph_dict)\n    num = len(paragraph_dict)\n    return json_str, num\ndef paragraph_to_text(raw_paragraph_list):\n    new_paragraph_list = []\n    for i, paragraph in enumerate(raw_paragraph_list):\n        paragraph_list = []\n        for sentence in paragraph:\n            sentence = ' '.join(sentence)\n            paragraph_list.append(sentence)\n        \n        new_paragraph = ' '.join(paragraph_list)\n        new_paragraph_list.append(new_paragraph)\n\n    paragraph_str = ' '.join(new_paragraph_list)\n    return paragraph_str\ndef summary_to_dict_str(summary_list):\n    summary_dict = {} # key = summary_id : value = summary sentence \n    for i, summary in enumerate(summary_list):\n        summary_dict[i] = ' '.join(summary)\n\n    json_str = json.dumps(summary_dict)\n    num = len(summary_dict)\n    return json_str, num\ndef summary_to_text(raw_summary_list):\n    summary_list = []\n    for i, summary in enumerate(raw_summary_list):\n        summary_list.append(' '.join(summary))\n\n    summary_str = ' '.join(summary_list)\n    return summary_str\ndef alter_json_data(json_list_data, filename=''):\n    new_json_list = []\n    for json_data in tqdm(json_list_data, desc=f'Altering json data {filename}'):\n        json_data = json_data.copy()\n        json_data['gold_labels'], _ = label_to_dict_str(json_data['gold_labels'])\n        json_data['news_text'] = paragraph_to_text(json_data['paragraphs'])\n        json_data['paragraphs'], num_paragraph = paragraph_to_dict_str(json_data['paragraphs'])\n        json_data['num_of_paragraphs'] = num_paragraph\n        json_data['summary_text'] = summary_to_text(json_data['summary'])\n        json_data['summary'], num_summary = summary_to_dict_str(json_data['summary'])\n        json_data['num_of_summary'] = num_summary\n        \n        new_json_list.append(json_data)\n    \n    return new_json_list\ndef create_dataset(jsonl):\n    header = list(jsonl[0].keys())\n    dataset_list = []\n    for json_data in jsonl:\n        row = []\n        for h in header:\n            row.append(json_data[h])\n        dataset_list.append(row)\n    \n    return header, dataset_list\ndef create_dataset_from_files(file_list):\n    df_header = None\n    dataset_list = []\n    for filename in file_list:\n        json_l = load_file_to_json_list(filename)\n        new_json_l = alter_json_data(json_l, filename)\n        header, dataset_part = create_dataset(new_json_l)\n        \n        if not df_header: df_header = header\n        dataset_list.extend(dataset_part)\n        \n    df_full = pd.DataFrame().from_records(dataset_list)\n    df_full = df_full.rename(columns=dict(enumerate(header)))\n    return df_full\ndf_train = create_dataset_from_files(train_files)\ndf_dev = create_dataset_from_files(dev_files)\ndf_test = create_dataset_from_files(test_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:25:26.788616Z","iopub.execute_input":"2025-04-07T09:25:26.789132Z","iopub.status.idle":"2025-04-07T09:25:38.763797Z","shell.execute_reply.started":"2025-04-07T09:25:26.789097Z","shell.execute_reply":"2025-04-07T09:25:38.762515Z"}},"outputs":[{"name":"stderr","text":"Loading data train.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14262/14262 [00:02<00:00, 6422.32it/s] \nAltering json data train.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14262/14262 [00:06<00:00, 2069.70it/s]\nLoading data dev.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 17391.92it/s]\nAltering json data dev.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 750/750 [00:00<00:00, 2078.97it/s]\nLoading data test.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3762/3762 [00:00<00:00, 17844.07it/s]\nAltering json data test.01.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3762/3762 [00:01<00:00, 2121.47it/s]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# Konversi DataFrame ke Dataset\ntest_dataset = Dataset.from_pandas(df_test[['category', 'news_text', 'summary_text']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:26:02.527458Z","iopub.execute_input":"2025-04-07T09:26:02.527769Z","iopub.status.idle":"2025-04-07T09:26:02.583383Z","shell.execute_reply.started":"2025-04-07T09:26:02.527746Z","shell.execute_reply":"2025-04-07T09:26:02.582441Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\"\"\"from datasets import Dataset, DatasetDict\n\n# Konversi DataFrame ke Dataset\ntrain_dataset = Dataset.from_pandas(df_train[['news_text', 'summary_text']])\ndev_dataset = Dataset.from_pandas(df_dev[['news_text', 'summary_text']])\ntest_dataset = Dataset.from_pandas(df_test[['news_text', 'summary_text']])\n\n# Gabungkan menjadi DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": dev_dataset,\n    \"test\": test_dataset\n})\n\n# Cek struktur dataset\nprint(dataset)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T15:00:34.769031Z","iopub.execute_input":"2025-04-06T15:00:34.769275Z","iopub.status.idle":"2025-04-06T15:00:35.243018Z","shell.execute_reply.started":"2025-04-06T15:00:34.769255Z","shell.execute_reply":"2025-04-06T15:00:35.242121Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['news_text', 'summary_text'],\n        num_rows: 14262\n    })\n    validation: Dataset({\n        features: ['news_text', 'summary_text'],\n        num_rows: 750\n    })\n    test: Dataset({\n        features: ['news_text', 'summary_text'],\n        num_rows: 3762\n    })\n})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\"\"\"df_train_h = df_train[df_train['category'] == 'hiburan']\ndf_dev_h = df_dev[df_dev['category'] == 'hiburan']\ndf_test_h = df_test[df_test['category'] == 'hiburan']\n\ntrain_dataset = Dataset.from_pandas(df_train_h[['news_text', 'summary_text']])\ndev_dataset = Dataset.from_pandas(df_dev_h[['news_text', 'summary_text']])\ntest_dataset = Dataset.from_pandas(df_test_h[['news_text', 'summary_text']])\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T15:00:35.244108Z","iopub.execute_input":"2025-04-06T15:00:35.244459Z","iopub.status.idle":"2025-04-06T15:00:35.303419Z","shell.execute_reply.started":"2025-04-06T15:00:35.244405Z","shell.execute_reply":"2025-04-06T15:00:35.302779Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n\n    # Other popular models!\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/Llama-3.3-70B\",\n    \"unsloth/mistral-7b-instruct-v0.3\",\n    \"unsloth/Phi-4\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3-1b-it-bnb-4bit\",\n    max_seq_length = 2048, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:15:29.044133Z","iopub.execute_input":"2025-04-07T09:15:29.044378Z","iopub.status.idle":"2025-04-07T09:16:21.939192Z","shell.execute_reply.started":"2025-04-07T09:15:29.044358Z","shell.execute_reply":"2025-04-07T09:16:21.938237Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nINFO 04-07 09:15:56 [__init__.py:239] Automatically detected platform cuda.\n==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/965M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f38e5cdf93242b6b16cff94ca296cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e3faa7a53b4f24bb7f308cc518e2e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00fd41bc8ab64f18b20508ae95b0fb00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6016416c6954faa99453be169462885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"087b36bb95784845b500d338ca864300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f6269c55ea4ba2a70f3650efcd5a63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"431504e8bb024a88a5818a5e0261b5e9"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"test_dataset[2]['news_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:27:33.586456Z","iopub.execute_input":"2025-04-07T09:27:33.586753Z","iopub.status.idle":"2025-04-07T09:27:33.592313Z","shell.execute_reply.started":"2025-04-07T09:27:33.586731Z","shell.execute_reply":"2025-04-07T09:27:33.591547Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'Jakarta, CNN Indonesia â€“ Meski sudah hampir 12 tahun berlalu, film Mean Girls merupakan salah satu film remaja sepanjang masa yang sulit untuk dilupakan. Tak heran kemudian, rumah produksi film yang dibintangi Lindsay Lohan, Rachel McAdams dan Amanda Seyfried ini kemudian memutuskan akan mengadaptasinya untuk dibuat dalam versi musikal. Konfirmasi akan kepastian adaptasi dari film yang naskah skenarionya ditulis Tina Fey dan disutradara Mark Waters ini diumumkan lewat akun resmi Twitter dan Facebook MeanGirlsDC. Dijadwalkan pementasan musikal dari adaptasi naskah itu akan berlangsung September, tahun depan di Washington DC. Kabar adaptasi ke musikal ini, seperti dikutip dari NME, muncul pada Senin (3 / 10), yang juga ditandai sebagai Mean Girls Day oleh para penggemarnya. Sebelumnya, keinginan untuk mengadaptasi Mean Girls ke musikal sudah muncul pada Maret lalu. Fey pada Metro mengatakan,\"Saya, suami saya, komposer Jeff Richmond, dan penulis lirik Nell Benjamin, akan bekerja sama untuk adaptasi musikal ini.\"Nell Benjamin sebelumnya juga pernah membuat musikal Legally Blonde. Kepastian nama tempat dan tanggal pementasan belum diinformasikan, akan tetapi pihak Mean Girls memastikan akan mengumumkannya lewat akun media sosial mereka dalam waktu dekat. Mean Girls Day Para penggemar film Mean Girls menetapkan 3 Oktober sebagai Mean Girls Day. Ini mengacu pada salah satu dialog yang disampaikan tokoh utamanya Cady Heron (yang diperankan Lindsay Lohan), ketika ditanyai laki-laki yang ia sukai Aaron Samuels (Jonathan Bennett) akan waktu, yang kemudian ia jawab,\"Ini 3 Oktober\". Kutipan tersebut menjadi salah satu adegan berkesan sepanjang film Mean Girls, yang kemudian dinobatkan para pencinta film sebagai Mean Girls Day. Lohan, pada Senin (3 / 10) juga turut merayakannya dengan membagi postingan cuplikan percakapannya, dan poster Mean Girls bertuliskan\"It \\'s October 3rd #NationalMeanGirlsDay\". Ungkapan itu ia sertai hashtag #whatdayisit, meski di saat yang sama ia sedang dirawat di rumah sakit karena insiden luka di bagian jari akibat jangkal kapal saat ia berliburan di Turki. (rsa)'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:27:40.566059Z","iopub.execute_input":"2025-04-07T09:27:40.566349Z","iopub.status.idle":"2025-04-07T09:27:40.570966Z","shell.execute_reply.started":"2025-04-07T09:27:40.566328Z","shell.execute_reply":"2025-04-07T09:27:40.570257Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def summarize_batch(batch):\n    # Buat list isi prompt\n    prompts = []\n    for text in batch[\"news_text\"]:\n        messages = [{\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": f\"Ringkaskan teks berikut:\\n\\n{text}\",\n            }]\n        }]\n        chat_input = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        prompts.append(chat_input)\n\n    # Tokenisasi batch\n    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    # Generate semua\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            temperature=1.0,\n            top_p=0.95,\n            top_k=64\n        )\n\n    # Decode semua\n    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return {\"model_base_generated\": results}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:27:57.029381Z","iopub.execute_input":"2025-04-07T09:27:57.029685Z","iopub.status.idle":"2025-04-07T09:27:57.035144Z","shell.execute_reply.started":"2025-04-07T09:27:57.029662Z","shell.execute_reply":"2025-04-07T09:27:57.034471Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\"\"\"import gc\n# selesai generate\n  # variabel hasil generate\ndel model, tokenizer\ngc.collect()\ntorch.cuda.empty_cache()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:36:34.082343Z","iopub.execute_input":"2025-04-06T16:36:34.082716Z","iopub.status.idle":"2025-04-06T16:36:34.971963Z","shell.execute_reply.started":"2025-04-06T16:36:34.082686Z","shell.execute_reply":"2025-04-06T16:36:34.970035Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nprint(used_memory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T15:30:39.278284Z","iopub.execute_input":"2025-04-06T15:30:39.278666Z","iopub.status.idle":"2025-04-06T15:30:39.285180Z","shell.execute_reply.started":"2025-04-06T15:30:39.278634Z","shell.execute_reply":"2025-04-06T15:30:39.284135Z"}},"outputs":[{"name":"stdout","text":"14.58\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:37:57.487199Z","iopub.execute_input":"2025-04-06T16:37:57.487547Z","iopub.status.idle":"2025-04-06T16:37:57.819198Z","shell.execute_reply.started":"2025-04-06T16:37:57.487518Z","shell.execute_reply":"2025-04-06T16:37:57.818117Z"}},"outputs":[{"name":"stdout","text":"Sun Apr  6 16:37:57 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   70C    P0             31W /   70W |    1103MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   57C    P0             29W /   70W |     123MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"df_test_h = df_test[df_test['category'] == 'hiburan']\ntest_dataset_h = Dataset.from_pandas(df_test_h[['category', 'news_text', 'summary_text']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:28:12.446878Z","iopub.execute_input":"2025-04-07T09:28:12.447232Z","iopub.status.idle":"2025-04-07T09:28:12.474254Z","shell.execute_reply.started":"2025-04-07T09:28:12.447202Z","shell.execute_reply":"2025-04-07T09:28:12.473360Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"test_dataset_h = test_dataset_h.map(\n    summarize_batch,\n    batched=True,\n    batch_size=64  # atau 8, 16, tergantung VRAM kamu\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:28:15.114101Z","iopub.execute_input":"2025-04-07T09:28:15.114380Z","iopub.status.idle":"2025-04-07T09:32:14.995266Z","shell.execute_reply.started":"2025-04-07T09:28:15.114359Z","shell.execute_reply":"2025-04-07T09:32:14.994527Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e8760e1c0744c139bb4a0d8a9aa7b6a"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = test_dataset_h.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:35:24.090324Z","iopub.execute_input":"2025-04-07T09:35:24.090651Z","iopub.status.idle":"2025-04-07T09:35:24.113919Z","shell.execute_reply.started":"2025-04-07T09:35:24.090629Z","shell.execute_reply":"2025-04-07T09:35:24.113232Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import re\n\ndef extract_only_summary(text):\n    \"\"\"\n    Menghapus bagian dari 'user' sampai 'model' (termasuk keduanya) dan hanya mengembalikan output model.\n    Cocok jika seluruh isi (user prompt + model output) disimpan dalam satu string.\n    \"\"\"\n    # Pola regex untuk menghapus bagian dari 'user' hingga 'model'\n    pattern = re.compile(r\"user.*?model\\s*\", re.DOTALL | re.IGNORECASE)\n    cleaned_text = re.sub(pattern, \"\", text).strip()\n    return cleaned_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:36:40.201191Z","iopub.execute_input":"2025-04-07T09:36:40.201626Z","iopub.status.idle":"2025-04-07T09:36:40.206751Z","shell.execute_reply.started":"2025-04-07T09:36:40.201588Z","shell.execute_reply":"2025-04-07T09:36:40.205660Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"df[\"model_base_generated\"] = df[\"model_base_generated\"].apply(extract_only_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:36:41.133930Z","iopub.execute_input":"2025-04-07T09:36:41.134250Z","iopub.status.idle":"2025-04-07T09:36:41.155493Z","shell.execute_reply.started":"2025-04-07T09:36:41.134225Z","shell.execute_reply":"2025-04-07T09:36:41.154850Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"df[\"model_base_generated\"][351]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:37:53.346390Z","iopub.execute_input":"2025-04-07T09:37:53.346750Z","iopub.status.idle":"2025-04-07T09:37:53.351763Z","shell.execute_reply.started":"2025-04-07T09:37:53.346720Z","shell.execute_reply":"2025-04-07T09:37:53.351111Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'.\\n\\n************************************************************************************************************************************************************************************************************************************************************'"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T09:35:31.122038Z","iopub.execute_input":"2025-04-07T09:35:31.122343Z","iopub.status.idle":"2025-04-07T09:35:31.149992Z","shell.execute_reply.started":"2025-04-07T09:35:31.122320Z","shell.execute_reply":"2025-04-07T09:35:31.148999Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"  category                                          news_text  \\\n0  hiburan  Jakarta, CNN Indonesia â€“ Dilansir AFP, seorang...   \n1  hiburan  Jakarta, CNN Indonesia â€“ Sebuah lagu misterius...   \n2  hiburan  Teh sudah jadi bagian yang tak terpisahkan dar...   \n3  hiburan  Seorang ibu di Massachusetts, Amerika Serikat,...   \n4  hiburan  Butuh waktu bertahun - tahu lamanya bagi seseo...   \n\n                                        summary_text  __index_level_0__  \\\n0  Eman Ahmed Abd El Aty memiliki berat badan men...                  0   \n1  Sebuah lagu misterius mendadak muncul di iTune...                  5   \n2  Teh memiliki fungsi istimewa yang ampuh mengur...                 14   \n3  Seorang ibu di Massachusetts, AS, memenangkan ...                 15   \n4  Ketekunan dan kegigihan akan menghasilkan kual...                 36   \n\n                                model_base_generated  \n0  user\\nRingkaskan teks berikut:\\n\\nJakarta, CNN...  \n1  user\\nRingkaskan teks berikut:\\n\\nJakarta, CNN...  \n2  user\\nRingkaskan teks berikut:\\n\\nTeh sudah ja...  \n3  user\\nRingkaskan teks berikut:\\n\\nSeorang ibu ...  \n4  user\\nRingkaskan teks berikut:\\n\\nButuh waktu ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>news_text</th>\n      <th>summary_text</th>\n      <th>__index_level_0__</th>\n      <th>model_base_generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hiburan</td>\n      <td>Jakarta, CNN Indonesia â€“ Dilansir AFP, seorang...</td>\n      <td>Eman Ahmed Abd El Aty memiliki berat badan men...</td>\n      <td>0</td>\n      <td>user\\nRingkaskan teks berikut:\\n\\nJakarta, CNN...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hiburan</td>\n      <td>Jakarta, CNN Indonesia â€“ Sebuah lagu misterius...</td>\n      <td>Sebuah lagu misterius mendadak muncul di iTune...</td>\n      <td>5</td>\n      <td>user\\nRingkaskan teks berikut:\\n\\nJakarta, CNN...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hiburan</td>\n      <td>Teh sudah jadi bagian yang tak terpisahkan dar...</td>\n      <td>Teh memiliki fungsi istimewa yang ampuh mengur...</td>\n      <td>14</td>\n      <td>user\\nRingkaskan teks berikut:\\n\\nTeh sudah ja...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hiburan</td>\n      <td>Seorang ibu di Massachusetts, Amerika Serikat,...</td>\n      <td>Seorang ibu di Massachusetts, AS, memenangkan ...</td>\n      <td>15</td>\n      <td>user\\nRingkaskan teks berikut:\\n\\nSeorang ibu ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hiburan</td>\n      <td>Butuh waktu bertahun - tahu lamanya bagi seseo...</td>\n      <td>Ketekunan dan kegigihan akan menghasilkan kual...</td>\n      <td>36</td>\n      <td>user\\nRingkaskan teks berikut:\\n\\nButuh waktu ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"import gc\n# selesai generate\n  # variabel hasil generate\ndel summarize_batch\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:12:03.966000Z","iopub.execute_input":"2025-04-06T17:12:03.966359Z","iopub.status.idle":"2025-04-06T17:12:05.327941Z","shell.execute_reply.started":"2025-04-06T17:12:03.966329Z","shell.execute_reply":"2025-04-06T17:12:05.325147Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:12:07.054079Z","iopub.execute_input":"2025-04-06T17:12:07.054417Z","iopub.status.idle":"2025-04-06T17:12:07.388091Z","shell.execute_reply.started":"2025-04-06T17:12:07.054388Z","shell.execute_reply":"2025-04-06T17:12:07.386920Z"}},"outputs":[{"name":"stdout","text":"Sun Apr  6 17:12:07 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   70C    P0             32W /   70W |    1931MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   56C    P0             29W /   70W |     123MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"from unsloth import FastModel\nfrom peft import PeftModel\n\n# 2. Pasang adapter hasil fine-tune\nadapter_path = \"/kaggle/input/adapter/kaggle/working/results/checkpoint-255/\"  # ganti sesuai lokasi adapter kamu\nmodel = PeftModel.from_pretrained(model, adapter_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:55:47.598939Z","iopub.execute_input":"2025-04-06T16:55:47.599316Z","iopub.status.idle":"2025-04-06T16:55:47.979352Z","shell.execute_reply.started":"2025-04-06T16:55:47.599288Z","shell.execute_reply":"2025-04-06T16:55:47.978472Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\ndef summarize_batch(batch):\n    # Buat list isi prompt\n    prompts = []\n    for text in batch[\"news_text\"]:\n        messages = [{\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": f\"Ringkaskan teks berikut:\\n\\n{text}\",\n            }]\n        }]\n        chat_input = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        prompts.append(chat_input)\n\n    # Tokenisasi batch\n    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    # Generate semua\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            temperature=1.0,\n            top_p=0.95,\n            top_k=64\n        )\n\n    # Decode semua\n    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return {\"model_finetune_generated\": results}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:12:25.617261Z","iopub.execute_input":"2025-04-06T17:12:25.617679Z","iopub.status.idle":"2025-04-06T17:12:25.628701Z","shell.execute_reply.started":"2025-04-06T17:12:25.617634Z","shell.execute_reply":"2025-04-06T17:12:25.627783Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"test_dataset_h = test_dataset_h.map(\n    summarize_batch,\n    batched=True,\n    batch_size=32  # atau 8, 16, tergantung VRAM kamu\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:12:29.276249Z","iopub.execute_input":"2025-04-06T17:12:29.276687Z","iopub.status.idle":"2025-04-06T17:17:57.840570Z","shell.execute_reply.started":"2025-04-06T17:12:29.276650Z","shell.execute_reply":"2025-04-06T17:17:57.839828Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7621bdf91db841aba868cac1c408f1a6"}},"metadata":{}}],"execution_count":153},{"cell_type":"code","source":"test_dataset_h_result = test_dataset_h.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:18:01.102408Z","iopub.execute_input":"2025-04-06T17:18:01.102737Z","iopub.status.idle":"2025-04-06T17:18:01.116762Z","shell.execute_reply.started":"2025-04-06T17:18:01.102713Z","shell.execute_reply":"2025-04-06T17:18:01.116021Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"print(test_dataset_h_result.iloc[2]['model_finetune_generated'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:19:33.506700Z","iopub.execute_input":"2025-04-06T17:19:33.507020Z","iopub.status.idle":"2025-04-06T17:19:33.512290Z","shell.execute_reply.started":"2025-04-06T17:19:33.506996Z","shell.execute_reply":"2025-04-06T17:19:33.511390Z"}},"outputs":[{"name":"stdout","text":"user\nRingkaskan teks berikut:\n\nTeh sudah jadi bagian yang tak terpisahkan dari kehidupan orang Indonesia . Jika tak percaya , coba perhatikan kembali minuman yang dipesan banyak orang setiap kali kamu makan di sebuah restoran . Jawaban yang terdengar pasti tak akan jauh dari yang namanya ' es teh manis ' , ' es teh tawar ' , atau ' teh manis hangat ' . Apakah kamu termasuk salah satunya ? Sejak dulu , teh memang dikenal sebagai minuman yang memiliki khasiat baik bagi kesehatan . Semua karena kandungan antioksidan , kafein , polifenol , dan vitaminnya yang beragam . Minuman ini juga ampuh untuk mengenyahkan racun dari dalam tubuh , lho . Jika rajin minum teh , kamu bisa terhindar dari sakit kepala dan kolesterol . Jantung dan pencernaan juga jadi lebih sehat . Dan hebatnya lagi , teh terbukti mampu memberi efek menenangkan secara psikologis bagi peminumnya . Masih meragukan khasiat baik teh bagi kesehatan manusia ? Khusus untuk kaum hawa pecinta teh , ada kabar gembira untuk kamu ! Karena rupanya , teh memiliki fungsi istimewa yang ampuh mengurangi risiko kanker payudara pada perempuan . Bagaimana bisa ? Semua dikarenakan kemampuan teh untuk mempengaruhi cara kerja gen dalam tubuh manusia . Dalam hal ini , yang jadi topik perbincangan adalah gen estrogen pada perempuan . Pada dasarnya , manusia terlahir dengan susunan dan kondisi gen yang berbeda-beda , dan tak semuanya berada dalam kondisi aktif . Jika kamu rajin mengonsumsi teh setiap hari , kebiasaan baik ini mampu menstimulasi aktivitas gen yang bertugas untuk menghambat produksi gen estrogen . Jika produksi estrogen terhambat , risiko kanker payudara otomatis berkurang . Hal ini tentunya berdampak baik bagi kaum hawa , karena produksi estrogen berlebihan merupakan salah satu penyebab terjadinya kanker payudara . Meski berdampak baik bagi kesehatan , namun konsumsi teh yang berlebihan juga tak baik bagi kesehatan . Karena kandungan kafein yang terdapat dalam teh bisa menghambat proses penyerapan zat besi oleh tubuh . Seperti dilansir Hello Sehat , lima cangkir merupakan jumlah teh maksimal yang boleh kamu konsumsi . Karena kelebihan konsumsi teh bisa menyebabkan insomnia bagi sebagian orang . Dan yang perlu diingat , jangan menambahkan gula terlalu banyak saat kamu mengonsumsi teh , karena bisa memicu terjadinya diabetes .\nmodel\nJika rajin minum teh , kamu bisa terhindar dari sakit kepala dan kolesterol . Jantung dan pencernaan juga jadi lebih sehat . Dan hebatnya , teh mampu memberi efek menenangkan secara psikologis bagi peminumnya . Khusus untuk kaum hawa , teh memiliki fungsi istimewa yang ampuh mengurangi risiko kanker payudara . Karena rupanya , teh memiliki fungsi istimewa yang ampuh mengurangi risiko kanker payudara .\n","output_type":"stream"}],"execution_count":163},{"cell_type":"code","source":"print(test_dataset_h_result.iloc[5]['summary_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:28:14.277492Z","iopub.execute_input":"2025-04-06T17:28:14.277824Z","iopub.status.idle":"2025-04-06T17:28:14.282896Z","shell.execute_reply.started":"2025-04-06T17:28:14.277802Z","shell.execute_reply":"2025-04-06T17:28:14.282019Z"}},"outputs":[{"name":"stdout","text":"Meditasi juga mampu membawa banyak manfaat menguntungkan untuk kesehatan fisik anda . Meditasi secara tidak langsung dapat berdampak pula pada penurunan tekanan darah anda . Banyak penelitian yang telah membuktikan bahwa meditasi juga mampu meningkatkan kesehatan pencernaan , dapat memperbaiki pola tidur , dapat meningkatkan kesuburan , dan juga dengan meditasi secara teratur maka dapat meningkatkan sistem kesehatan internal tubuh .\n","output_type":"stream"}],"execution_count":167},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [{\n        \"type\" : \"text\",\n        \"text\" : \"\"\"Ringkaskan teks berikut:\\n\\n 'Jakarta , CNN Indonesia - - Sebuah lagu misterius mendadak muncul di iTunes akhir pekan lalu . Lagu itu berjudul Behind Me , disebut sebagai karya dari seorang artis tak terkenal bernama Guido Dos Santos . Lagu itu diperkenalkan ke iTunes tanpa keterangan apa pun , lalu dihapus . Namun Gay Times melaporkan , lagu itu sempat membuat tertarik banyak orang . Bahkan ia masuk daftar 50 besar lagu laris didengar di delapan negara , meski tak disebutkan mana saja . Mengutip Independent , lagu itu dipercaya merupakan versi demo dari karya Madonna yang berjudul Two Steps Behind Me . Itu merupakan demo yang direkam Madonna pada 2015 untuk album Rebel Heart . Saat itu Two Steps Behind Me disebut berkaitan dengan Lady Gaga . Namun manajer Madonna , Guy Oseary menolak gagasan itu . â€œ Lagu itu bukan tentang Gaga atau siapa pun , â€ tulisnya di Twitter pada 2014 . Oseary maupun Madonna tidak berkata apa-apa tentang lagu pendek Behind Me yang baru muncul di iTunes . Alih - alih , ia justru merespons perbincangan soal biopiknya , Blond Ambition . Ia tidak merestui film yang dibikin tanpa persetujuannya itu . Madonna menulis , â€œ Tidak seorang pun tahu apa yang saya ketahui dan yang saya lihat . Hanya saya yang bisa mengisahkan cerita hidup saya . Orang lain yang mencobanya hanya terlihat mencari keuntungan dan bodoh , mencari gratifikasi tanpa bekerja apa-apa , â€ tulisnya . Tidak ada pemberitaan lebih lanjut bagaimana akhirnya nasib film itu .'\"\"\",\n    }]\n}]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt = True, # Must add for generation\n)\noutputs = model.generate(\n    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 128, # Increase for longer outputs!\n    # Recommended Gemma-3 settings!\n    temperature = 1.0, top_p = 0.95, top_k = 64,\n)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:00:58.393947Z","iopub.execute_input":"2025-04-06T18:00:58.394345Z","iopub.status.idle":"2025-04-06T18:01:11.053850Z","shell.execute_reply.started":"2025-04-06T18:00:58.394315Z","shell.execute_reply":"2025-04-06T18:01:11.052878Z"}},"outputs":[{"execution_count":197,"output_type":"execute_result","data":{"text/plain":"[\"<bos><bos><start_of_turn>user\\nRingkaskan teks berikut:\\n\\n 'Jakarta , CNN Indonesia - - Sebuah lagu misterius mendadak muncul di iTunes akhir pekan lalu . Lagu itu berjudul Behind Me , disebut sebagai karya dari seorang artis tak terkenal bernama Guido Dos Santos . Lagu itu diperkenalkan ke iTunes tanpa keterangan apa pun , lalu dihapus . Namun Gay Times melaporkan , lagu itu sempat membuat tertarik banyak orang . Bahkan ia masuk daftar 50 besar lagu laris didengar di delapan negara , meski tak disebutkan mana saja . Mengutip Independent , lagu itu dipercaya merupakan versi demo dari karya Madonna yang berjudul Two Steps Behind Me . Itu merupakan demo yang direkam Madonna pada 2015 untuk album Rebel Heart . Saat itu Two Steps Behind Me disebut berkaitan dengan Lady Gaga . Namun manajer Madonna , Guy Oseary menolak gagasan itu . â€œ Lagu itu bukan tentang Gaga atau siapa pun , â€ tulisnya di Twitter pada 2014 . Oseary maupun Madonna tidak berkata apa-apa tentang lagu pendek Behind Me yang baru muncul di iTunes . Alih - alih , ia justru merespons perbincangan soal biopiknya , Blond Ambition . Ia tidak merestui film yang dibikin tanpa persetujuannya itu . Madonna menulis , â€œ Tidak seorang pun tahu apa yang saya ketahui dan yang saya lihat . Hanya saya yang bisa mengisahkan cerita hidup saya . Orang lain yang mencobanya hanya terlihat mencari keuntungan dan bodoh , mencari gratifikasi tanpa bekerja apa-apa , â€ tulisnya . Tidak ada pemberitaan lebih lanjut bagaimana akhirnya nasib film itu .'<end_of_turn>\\n<start_of_turn>model\\nSebuah lagu misterius mendadak muncul di iTunes akhir pekan lalu . Lagu itu berjudul Behind Me , disebut sebagai karya seorang artis tak terkenal bernama Guido Dos Santos . Lagu itu diperkenalkan ke iTunes tanpa keterangan apa pun , lalu dihapus . Namun Gay Times melaporkan , lagu itu sempat membuat tertarik banyak orang . Bahkan ia masuk daftar 50 besar lagu laris didengar di delapan negara , meski tak disebutkan mana saja .<end_of_turn>\"]"},"metadata":{}}],"execution_count":197},{"cell_type":"code","source":"import gc\n# selesai generate\n  # variabel hasil generate\ndel summarize_batch, model, tokenizer\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:29:30.920256Z","iopub.execute_input":"2025-04-06T17:29:30.920683Z","iopub.status.idle":"2025-04-06T17:29:31.985056Z","shell.execute_reply.started":"2025-04-06T17:29:30.920652Z","shell.execute_reply":"2025-04-06T17:29:31.983911Z"}},"outputs":[],"execution_count":168},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:29:47.139077Z","iopub.execute_input":"2025-04-06T17:29:47.139376Z","iopub.status.idle":"2025-04-06T17:29:47.478718Z","shell.execute_reply.started":"2025-04-06T17:29:47.139354Z","shell.execute_reply":"2025-04-06T17:29:47.477562Z"}},"outputs":[{"name":"stdout","text":"Sun Apr  6 17:29:47 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   69C    P0             31W /   70W |    1963MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   66C    P0             31W /   70W |     123MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":169},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\n\n\ndef summarize_batch(batch):\n    # Buat list isi prompt\n    prompts = []\n    for text in batch[\"news_text\"]:\n        messages = [{\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": f\"\"\"\n                Bacalah teks berita berikut ini.\nTugas Anda adalah menuliskan ringkasan dari teks tersebut secara padat dan langsung ke inti informasi, menggunakan gaya bahasa berita harian, tanpa tambahan pembuka seperti Ringkasan: atau Kesimpulan:, dan hanya terdiri dari dua sampai lima kalimat pendek.\n\\n\\n{text}\"\"\",\n            }]\n        }]\n        chat_input = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        prompts.append(chat_input)\n\n    # Tokenisasi batch\n    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    # Generate semua\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            temperature=1.0,\n            top_p=0.95,\n            top_k=64\n        )\n\n    # Decode semua\n    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return {\"model_base_zeroshot_generated\": results}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:32:22.686771Z","iopub.execute_input":"2025-04-06T17:32:22.687127Z","iopub.status.idle":"2025-04-06T17:32:22.694328Z","shell.execute_reply.started":"2025-04-06T17:32:22.687098Z","shell.execute_reply":"2025-04-06T17:32:22.693515Z"}},"outputs":[],"execution_count":172},{"cell_type":"code","source":"test_dataset_h = test_dataset_h.map(\n    summarize_batch,\n    batched=True,\n    batch_size=32  # atau 8, 16, tergantung VRAM kamu\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:32:51.896145Z","iopub.execute_input":"2025-04-06T17:32:51.896469Z","iopub.status.idle":"2025-04-06T17:37:21.686420Z","shell.execute_reply.started":"2025-04-06T17:32:51.896423Z","shell.execute_reply":"2025-04-06T17:37:21.685666Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4e9204b42f42c298410f368a4780b7"}},"metadata":{}}],"execution_count":173},{"cell_type":"code","source":"test_dataset_h_result = test_dataset_h.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:38:05.813935Z","iopub.execute_input":"2025-04-06T17:38:05.814334Z","iopub.status.idle":"2025-04-06T17:38:05.829624Z","shell.execute_reply.started":"2025-04-06T17:38:05.814303Z","shell.execute_reply":"2025-04-06T17:38:05.828720Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"import gc\n# selesai generate\n  # variabel hasil generate\ndel results\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:03:11.125907Z","iopub.execute_input":"2025-04-06T18:03:11.126222Z","iopub.status.idle":"2025-04-06T18:03:11.154693Z","shell.execute_reply.started":"2025-04-06T18:03:11.126196Z","shell.execute_reply":"2025-04-06T18:03:11.153509Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-202-7590ea0c409f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# selesai generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# variabel hasil generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"],"ename":"NameError","evalue":"name 'results' is not defined","output_type":"error"}],"execution_count":202},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:03:14.552224Z","iopub.execute_input":"2025-04-06T18:03:14.552559Z","iopub.status.idle":"2025-04-06T18:03:14.885621Z","shell.execute_reply.started":"2025-04-06T18:03:14.552533Z","shell.execute_reply":"2025-04-06T18:03:14.884577Z"}},"outputs":[{"name":"stdout","text":"Sun Apr  6 18:03:14 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   69C    P0             31W /   70W |    2795MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   59C    P0             30W /   70W |     123MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":203},{"cell_type":"code","source":"# 2. Pasang adapter hasil fine-tune\nadapter_path = \"/kaggle/input/adapter172/kaggle/working/results/checkpoint-172/\"  # ganti sesuai lokasi adapter kamu\nmodel = PeftModel.from_pretrained(model, adapter_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:40:26.593809Z","iopub.execute_input":"2025-04-06T17:40:26.594175Z","iopub.status.idle":"2025-04-06T17:40:27.511471Z","shell.execute_reply.started":"2025-04-06T17:40:26.594147Z","shell.execute_reply":"2025-04-06T17:40:27.510411Z"}},"outputs":[],"execution_count":179},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\ndef summarize_batch(batch):\n    # Buat list isi prompt\n    prompts = []\n    for text in batch[\"news_text\"]:\n        messages = [{\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": f\"Ringkaskan teks berikut:\\n\\n{text}\",\n            }]\n        }]\n        chat_input = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        prompts.append(chat_input)\n\n    # Tokenisasi batch\n    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    # Generate semua\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            temperature=1.0,\n            top_p=0.95,\n            top_k=64\n        )\n\n    # Decode semua\n    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    return {\"model_finetune172_generated\": results}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:03:22.782067Z","iopub.execute_input":"2025-04-06T18:03:22.782574Z","iopub.status.idle":"2025-04-06T18:03:22.791604Z","shell.execute_reply.started":"2025-04-06T18:03:22.782535Z","shell.execute_reply":"2025-04-06T18:03:22.790801Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset_h = test_dataset_h.map(\n    summarize_batch,\n    batched=True,\n    batch_size=8  # atau 8, 16, tergantung VRAM kamu\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:03:33.031074Z","iopub.execute_input":"2025-04-06T18:03:33.031409Z","iopub.status.idle":"2025-04-06T18:19:20.861226Z","shell.execute_reply.started":"2025-04-06T18:03:33.031382Z","shell.execute_reply":"2025-04-06T18:19:20.860306Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/355 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47f129fd854c48be9b075a468ce4211d"}},"metadata":{}}],"execution_count":205},{"cell_type":"code","source":"test_dataset_h_result = test_dataset_h.to_pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:20:40.231702Z","iopub.execute_input":"2025-04-06T18:20:40.232021Z","iopub.status.idle":"2025-04-06T18:20:40.249304Z","shell.execute_reply.started":"2025-04-06T18:20:40.231998Z","shell.execute_reply":"2025-04-06T18:20:40.248533Z"}},"outputs":[],"execution_count":206},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset_h_result['model_finetune172_generated'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:21:17.432512Z","iopub.execute_input":"2025-04-06T18:21:17.432842Z","iopub.status.idle":"2025-04-06T18:21:17.438590Z","shell.execute_reply.started":"2025-04-06T18:21:17.432814Z","shell.execute_reply":"2025-04-06T18:21:17.437846Z"}},"outputs":[{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"'user\\nRingkaskan teks berikut:\\n\\nJakarta , CNN Indonesia - - Sebuah lagu misterius mendadak muncul di iTunes akhir pekan lalu . Lagu itu berjudul Behind Me , disebut sebagai karya dari seorang artis tak terkenal bernama Guido Dos Santos . Lagu itu diperkenalkan ke iTunes tanpa keterangan apa pun , lalu dihapus . Namun Gay Times melaporkan , lagu itu sempat membuat tertarik banyak orang . Bahkan ia masuk daftar 50 besar lagu laris didengar di delapan negara , meski tak disebutkan mana saja . Mengutip Independent , lagu itu dipercaya merupakan versi demo dari karya Madonna yang berjudul Two Steps Behind Me . Itu merupakan demo yang direkam Madonna pada 2015 untuk album Rebel Heart . Saat itu Two Steps Behind Me disebut berkaitan dengan Lady Gaga . Namun manajer Madonna , Guy Oseary menolak gagasan itu . â€œ Lagu itu bukan tentang Gaga atau siapa pun , â€ tulisnya di Twitter pada 2014 . Oseary maupun Madonna tidak berkata apa-apa tentang lagu pendek Behind Me yang baru muncul di iTunes . Alih - alih , ia justru merespons perbincangan soal biopiknya , Blond Ambition . Ia tidak merestui film yang dibikin tanpa persetujuannya itu . Madonna menulis , â€œ Tidak seorang pun tahu apa yang saya ketahui dan yang saya lihat . Hanya saya yang bisa mengisahkan cerita hidup saya . Orang lain yang mencobanya hanya terlihat mencari keuntungan dan bodoh , mencari gratifikasi tanpa bekerja apa-apa , â€ tulisnya . Tidak ada pemberitaan lebih lanjut bagaimana akhirnya nasib film itu .\\nmodel\\n . Sebuah lagu misterius muncul di iTunes akhir pekan lalu . Lagu itu berjudul Behind Me , disebut sebagai karya seorang artis tak terkenal bernama Guido Dos Santos . Lagu itu diperkenalkan ke iTunes tanpa keterangan apa pun , lalu dihapus . Namun Gay Times melaporkan , lagu itu sempat membuat tertarik banyak orang . Bahkan ia masuk daftar 50 besar lagu laris didengar di delapan negara , meski tak disebutkan mana saja .'"},"metadata":{}}],"execution_count":208},{"cell_type":"code","source":"test_dataset_h['summary_text'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T17:48:20.018653Z","iopub.execute_input":"2025-04-06T17:48:20.018978Z","iopub.status.idle":"2025-04-06T17:48:20.025664Z","shell.execute_reply.started":"2025-04-06T17:48:20.018954Z","shell.execute_reply":"2025-04-06T17:48:20.024724Z"}},"outputs":[{"execution_count":187,"output_type":"execute_result","data":{"text/plain":"'Eman Ahmed Abd El Aty memiliki berat badan mencapai 500 kilogram sebelum menjalankan operasi di Mumbai Maret lalu dimana ia mengurangi seperlima dari berat badannya . Abd El Aty diberi diet cairan khusus selama berada di India yang bertujuan menurunkan berat badan . Kini , berat badannya telah turun drastis sebanyak 323 kilogram dalam tiga bulan . Sekarang berat badannya tinggal 176,6 kilogram .'"},"metadata":{}}],"execution_count":187},{"cell_type":"code","source":"test_dataset_h_result.to_csv('data_hasil_inference')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:21:37.039879Z","iopub.execute_input":"2025-04-06T18:21:37.040215Z","iopub.status.idle":"2025-04-06T18:21:37.151707Z","shell.execute_reply.started":"2025-04-06T18:21:37.040190Z","shell.execute_reply":"2025-04-06T18:21:37.151003Z"}},"outputs":[],"execution_count":209},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}